{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Suhan Shetty , suhan.n.shetty@gmail.com*\n",
    "\n",
    "*Project Files: [Google Drive](https://drive.google.com/open?id=13_9sMv5LcyGX5oo29x41sUwFhXkNbgWV)*\n",
    "\n",
    "*Note: If you are using Google-Colaboratory, you can use the 'model_for_google_colab.ipynb' file that is included in the above-mentioned link. 'model_for_google_colab.ipynb' is customized to work in Google-Colaboratory, however, I have not given description in that notebook. For the details, follow this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end Learning  for Self-driving Cars  \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, I will walk you through the process of designing an end-to-end learning model for a self-driving car using the resources provided by [Udacity](https://in.udacity.com/course/self-driving-car-engineer-nanodegree--nd013). We will use the tools provided by Udacity to the public. I have customized some of those files, and I have shared all the necessary files in this [google-drive](https://drive.google.com/open?id=13_9sMv5LcyGX5oo29x41sUwFhXkNbgWV). You can use them to follow along. \n",
    "\n",
    "You can go through the following resources to understand the concept of end-to-end learning: \n",
    "- [Siraj's Youtube Tutorial](https://www.youtube.com/watch?v=EaY5QiZwSP4)  \n",
    "- [NVDIA's Research Paper](https://arxiv.org/pdf/1604.07316.pdf)\n",
    "- [Udacity Behavioral Cloning Project](https://github.com/udacity/CarND-Behavioral-Cloning-P3)\n",
    "\n",
    "### Overview of the tutorial:\n",
    "- Project Environment Setup\n",
    "\n",
    "- Data collection and Preprocessing \n",
    "\n",
    "- Data Augmentation\n",
    "\n",
    "- Design CNN to Clone Car's Behavior\n",
    "\n",
    "- Testing the Performance of the Learning Model\n",
    "\n",
    "    \n",
    "   \n",
    "## Project Environment Setup\n",
    "\n",
    "   We will use a [Unity](https://unity3d.com/) based simulator provided by Udacity to collect data. The previously mentioned [video](https://www.youtube.com/watch?v=EaY5QiZwSP4) has already explained to you about the data collection procedure involved here. I have used Anaconda distribution in my Mac-OS-X to carry out the project. So, some of the details provided here apply only if you are using Anaconda distribution. \n",
    "\n",
    "First of all, download the file stored [here](link). The downloaded folder will be your project directory. The folder is named *e2e_project*, and I will assume it to be the current working directory in your PC if you want to follow along.\n",
    "\n",
    "**A brief description of the important files in the above-mentioned folder:**\n",
    "- **drive.py** : Script used to execute the simulator in autonomous mode (provided by Udacity)\n",
    "- **model.py** : Script used for training CNN model\n",
    "- **model.h5** : Saved Keras model of the trained CNN model\n",
    "- **data.py** : Contains the definition of preprocessing operations involved on the images obtained from simualtor\n",
    "- **data.zip** : The dataset used to train CNN model\n",
    "- **CarND-Term1-Starter-Kit-master** : Files required to setup the necessary python environment \n",
    "\n",
    "\n",
    "\n",
    "**Follow the below procedure to setup the project environment using the files provided in *e2e_project*:**\n",
    " 1. Install [Unity](https://unity3d.com/)\n",
    "\n",
    " \n",
    " 2. Create the Python envirnoment *carnd-term1*.\n",
    " \n",
    "    You can create this environment using the *environment.yml* file in *e2e_project/CarND-Term1-Starter-Kit/*. \n",
    "    You can refer to [this](https://github.com/udacity/CarND-Term1-Starter-Kit) page for details.\n",
    "    \n",
    "       \n",
    " 3. Install and open the simulator. \n",
    "    The simulator is included in 'e2e_project/simulator/'.  \n",
    "    \n",
    " \n",
    " 4. If you are using carnd-term1 environment to train the CNN then this step is not necessary. If that is not the case, then you need to match the verions of the environments that you have used in training with *carnd-term1* environment. Update Keras and tensorflow to the versions that are used in python environment that is used for training the CNN. I did CNN training using [Google Colaboratory](https://colab.research.google.com/) which used Keras-2.1.6 and Tensorflow-1.9.0-rc1. \n",
    " In Mac OS-X, I did the following to update keras and tensorflow of 'carnd-term1' environment:\n",
    " \n",
    " Open the terminal and execute the following:\n",
    " ```sh\n",
    "        >>source activate carnd-term1\n",
    "        >>pip install tensorflow==1.9.0-rc1\n",
    "        >>pip install keras==2.6.1 \n",
    "```\n",
    "   \n",
    "        \n",
    "  \n",
    " \n",
    "  \n",
    "\n",
    " \n",
    " \n",
    "## Data Collection and Preprocessing\n",
    "\n",
    "When you open the simulator you are given two options: Training Mode and Autonomous mode. For training mode, or just to play with the tool, you can directly open the simulator and click on 'training'. The training mode helps us in getting training dataset. You can drive the car using left and right arrow keys in your keyboard to control steering angle, up and down arrow keys to control speed.  You can collect data by recording the ride. You can start or stop recording of the ride by clicking on the key 'R'. For each frame in the recording mode, the simulator captures and saves three images taken from three cameras- left, right, and center, attached on the car. At the end of the recording, a 'driving_log.csv' file is created wherein the information about the image, steering angle, brake, speed of the car used in every frame of the recording. We use this dataset to train a CNN to model the behavior of the car and predict steering angle, given an image that is taken from the car.\n",
    "\n",
    " The autonomous mode is used to test our trained model. Once you have trained your CNN we can test its performance in the autonomous mode. In the autonomous mode, simulator is operated using *drive.py*. This script will get the car's camera image at every frame from the simulator and it will send the steering angle control output predicted from the trained CNN model to the simulator. For this tutorial, I have modified the script *drive.py* originally provided by Udacity to customize for this tutorial, and it is included in *e2e_project* directory. \n",
    " [image1]: ./tutorial_files/simulator.png \"Simualator\"\n",
    " ![alt text][image1]\n",
    "\n",
    "Using the simulator, I collected data from about 30 minutes of a ride. Here is a snapshot of the distribution of the data that I collected:\n",
    "\n",
    "[image2]: ./tutorial_files/original_data.png \"Raw data\"\n",
    "![image2]\n",
    "\n",
    "You can observe that the data is heavily biased towards zero steering angle. So we have to preprocess the data to remove this bias. \n",
    "\n",
    "I did some data scrapping and the pre-processed dataset looks as follows:\n",
    "[image3]: ./tutorial_files/balanced_data.png \"Preprocessed data\"\n",
    "![image3]\n",
    "\n",
    "As you can see, the processed data has a better distribution than the raw dataset. However, the distribution that I have obtained is not the best. We can get better distribution if we spend a bit more time in collecting the data and preprocessing.\n",
    "\n",
    "You can either collect your own dataset to follow along with this article, or, use the pre-processed dataset included in *e2e_project* directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the project and data path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "local_project_path = '' # Current folder - e2e_project\n",
    "local_data_path = os.path.join(local_project_path, 'data') \n",
    "sys.path.append(local_project_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visulaization \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "src_data_path = os.path.abspath(local_data_path)\n",
    "df = pd.read_csv(os.path.join(src_data_path,'driving_log.csv'))\n",
    "print(\"Number of data points: \", len(df))\n",
    "\n",
    "# We only consider magnitude since data augmentation will take care of balancing data across sign\n",
    "plt.hist(df.steering, bins = 30)\n",
    "plt.xlabel('steering')\n",
    "plt.show()\n",
    "\n",
    "# Note : The preprocessing was not done that well. More careful preprocessing would result in better dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11040,
     "status": "ok",
     "timestamp": 1530589722957,
     "user": {
      "displayName": "Suhan Shetty",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108658629281572684125"
     },
     "user_tz": -330
    },
    "id": "XDaI_LesjqjX",
    "outputId": "4ac308dc-14f2-42f4-85c5-f8b2b2b179fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version:  2.1.6\n",
      "Tensorflow Version:  1.9.0-rc1\n"
     ]
    }
   ],
   "source": [
    "# Install the packages required for training\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import skimage.transform as sktransform\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "from keras.callbacks import Callback\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.models import load_model\n",
    "import math\n",
    "\n",
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "print(\"Keras Version: \",keras.__version__)\n",
    "print(\"Tensorflow Version: \",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "We will first define the data augmentation techniques, and then we will train the CNN. For data augmentation I have referred to [this](https://navoshta.com/end-to-end-deep-learning/) blog. For more information on the data augmention techniques used here, refer to the blog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BIiBkQHm6bjn"
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "# Cameras we will use\n",
    "cameras = ['left', 'center', 'right']\n",
    "cameras_steering_correction = [.25, 0., -.25]\n",
    "\n",
    "def preprocess(image, top_offset=.375, bottom_offset=.125):\n",
    "    \"\"\"\n",
    "    Applies preprocessing pipeline to an image: crops `top_offset` and `bottom_offset`\n",
    "    portions of image, resizes to 32x128 px and scales pixel values to [0, 1].\n",
    "    \"\"\"\n",
    "    top = int(top_offset * image.shape[0])\n",
    "    bottom = int(bottom_offset * image.shape[0])\n",
    "    image = sktransform.resize(image[top:-bottom, :], (32, 128, 3))\n",
    "    return image\n",
    "\n",
    "def generate_samples(data,root_path, bs=128,augment=True):\n",
    "    \"\"\"\n",
    "    Keras generator yielding batches of training/validation data.\n",
    "    Applies data augmentation pipeline if `augment` is True.\n",
    "    \"\"\"\n",
    "    \n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    while True:\n",
    "        # Generate random batch of indices\n",
    "        indices = np.random.permutation(data.count()[0])\n",
    "        batch_size = bs #128\n",
    "\n",
    "        for batch in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[batch:(batch + batch_size)]\n",
    "            # Output arrays\n",
    "            x = np.empty([0, 32, 128, 3], dtype=np.float32)\n",
    "            y = np.empty([0], dtype=np.float32)\n",
    "            # Read in and preprocess a batch of images\n",
    "\n",
    "            for i in batch_indices:\n",
    "                # Randomly select camera\n",
    "                camera = np.random.randint(len(cameras)) if augment else 1\n",
    "                # Read frame image and work out steering angle\n",
    "                image = mpimg.imread(os.path.join(root_path, data[cameras[camera]].values[i].strip()))\n",
    "                image.setflags(write=1)\n",
    "                #print('Flags: ',image.flags)\n",
    "                angle = data.steering.values[i] + cameras_steering_correction[camera]\n",
    "                if augment:\n",
    "                    # Add random shadow as a vertical slice of image\n",
    "                    h, w = image.shape[0], image.shape[1]\n",
    "                    [x1, x2] = np.random.choice(w, 2, replace=False)\n",
    "                    k = h / (x2 - x1)\n",
    "                    b = - k * x1\n",
    "                    #print('Flags: ',image.flags)\n",
    "                    for i in range(h):\n",
    "                        c = int((i - b) / k)\n",
    "                        image[i, :c, :] = (image[i, :c, :] * .5).astype(np.int32)\n",
    "                        \n",
    "                # Randomly shift up and down while preprocessing\n",
    "                v_delta = .05 if augment else 0\n",
    "                image = preprocess(\n",
    "                    image,\n",
    "                    top_offset=random.uniform(.375 - v_delta, .375 + v_delta),\n",
    "                    bottom_offset=random.uniform(.125 - v_delta, .125 + v_delta)\n",
    "                )\n",
    "                # Append to batch\n",
    "                x = np.append(x, [image], axis=0)\n",
    "                y = np.append(y, [angle])\n",
    "                \n",
    "            # Randomly flip half of images in the batch.\n",
    "            \n",
    "            flip_indices = random.sample(range(x.shape[0]), int(x.shape[0] / 2))\n",
    "            x[flip_indices] = x[flip_indices, :, ::-1, :]\n",
    "            y[flip_indices] = -y[flip_indices]\n",
    "            \n",
    "            yield (x, y)\n",
    "            \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1530589733341,
     "user": {
      "displayName": "Suhan Shetty",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108658629281572684125"
     },
     "user_tz": -330
    },
    "id": "QoseJMIF3UyE",
    "outputId": "0ff9cd87-2c91-428a-e037-e4a6283e30f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  6219\n",
      "validation:  691\n"
     ]
    }
   ],
   "source": [
    "# Split Training and Validation data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Read the data\n",
    "    df = pd.io.parsers.read_csv(os.path.join(local_data_path, 'driving_log.csv'))\n",
    "    # Split data into training and validation sets\n",
    "    df_train, df_valid = model_selection.train_test_split(df, test_size=.1)\n",
    "\n",
    "print(\"train: \",df_train.shape[0])\n",
    "print(\"validation: \",df_valid.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Design and Training\n",
    "\n",
    "**CNN architectural consideration: **\n",
    "\n",
    "- I have used [densenet](https://arxiv.org/pdf/1608.06993.pdf) as building block of CNN\n",
    "\n",
    "- Two densenet blocks are used. The GPU used in Google Colaboratory had a limitation of a maximum of 128 stacking of feature map (depth of a layer). So, I have addressed this constraint in designing each dense block by limiting growth rate and number layers in each densenet block.\n",
    "\n",
    "\n",
    "- The layers are designed so that the receptive field is 'good' enough at the latter layers so that last few layers have seen the entire image once.\n",
    "\n",
    "\n",
    "- **1xC-Cx1** convolution is used instead of direct CxC-convolution. This will speed up the training without much loss in accuracy.\n",
    "\n",
    "\n",
    "- **Dilated convolution** is used. In case of autonomous driving the features of interest are mostly bounded regions which are better captured at a global level. Thus, we need to grow the receptive field fast enough so that the latter layers have a better global picture. I have used a dilatation rate of 2 in each convolution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1530589764399,
     "user": {
      "displayName": "Suhan Shetty",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108658629281572684125"
     },
     "user_tz": -330
    },
    "id": "QRz_yLVZC6r9",
    "outputId": "98e5dd6d-b6bd-4334-f566-cd22e9727c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network building blocks are ready\n"
     ]
    }
   ],
   "source": [
    "#  Building blocks for Densenet: https://arxiv.org/pdf/1608.06993.pdf\n",
    "\n",
    "def add_denseblock(input, growth_rate = 12, numLayers=12, dropout_rate = 0.2):\n",
    "    temp = input\n",
    "    for _ in range(numLayers):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        # 1xCx1 convolution with dilatation\n",
    "        conv1xC = Conv2D(growth_rate,(1,3), use_bias=False ,padding='same',dilation_rate=2)(relu)\n",
    "        conv1xCx1 = Conv2D(growth_rate,(3,1), use_bias=False ,padding='same',dilation_rate=2)(conv1xC)\n",
    "        concat = Concatenate(axis=-1)([temp,conv1xCx1])\n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "  \n",
    "def add_transition(input, growth = 16, dropout_rate = 0.2):\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    # 1x1 convolution\n",
    "    Conv2D_BottleNeck = Conv2D(growth, (1,1), use_bias=False ,padding='same',dilation_rate=2)(relu)\n",
    "    if dropout_rate>0:\n",
    "        Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg\n",
    "\n",
    "\n",
    "def output_layer(input):\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "\n",
    "    output = Dense(1,activation='tanh')(flat)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "print(\"Network building blocks are ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xR28Gl7bxjLw"
   },
   "outputs": [],
   "source": [
    "# Keeps track of model weights by saving them at the end of each epoch.\n",
    "\n",
    "class WeightsLogger(Callback):\n",
    "    def __init__(self, root_path):\n",
    "      super(WeightsLogger, self).__init__()\n",
    "      self.weights_root_path = os.path.join(root_path, 'weights/')\n",
    "      shutil.rmtree(self.weights_root_path, ignore_errors=True)\n",
    "      os.makedirs(self.weights_root_path, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.model.save_weights(os.path.join(self.weights_root_path, 'model_epoch_{}.h5'.format(epoch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 4012
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4566,
     "status": "ok",
     "timestamp": 1530594077095,
     "user": {
      "displayName": "Suhan Shetty",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108658629281572684125"
     },
     "user_tz": -330
    },
    "id": "_R0A16yEiykz",
    "outputId": "cbac0a16-a6ee-4d28-f30f-dac781bbdd80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 32, 128, 3)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 32, 128, 14)  378         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 32, 128, 14)  56          conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 32, 128, 14)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 32, 128, 14)  588         activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 32, 128, 14)  588         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 32, 128, 28)  0           conv2d_114[0][0]                 \n",
      "                                                                 conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 32, 128, 28)  112         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 32, 128, 28)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 32, 128, 14)  1176        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 32, 128, 14)  588         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 32, 128, 42)  0           concatenate_52[0][0]             \n",
      "                                                                 conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 32, 128, 42)  168         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 32, 128, 42)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 32, 128, 14)  1764        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 32, 128, 14)  588         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 32, 128, 56)  0           concatenate_53[0][0]             \n",
      "                                                                 conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 32, 128, 56)  224         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 32, 128, 56)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 32, 128, 14)  2352        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 32, 128, 14)  588         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 32, 128, 70)  0           concatenate_54[0][0]             \n",
      "                                                                 conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 32, 128, 70)  280         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 32, 128, 70)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 32, 128, 14)  2940        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 32, 128, 14)  588         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 32, 128, 84)  0           concatenate_55[0][0]             \n",
      "                                                                 conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 32, 128, 84)  336         concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 128, 84)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 32, 128, 14)  3528        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 32, 128, 14)  588         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 32, 128, 98)  0           concatenate_56[0][0]             \n",
      "                                                                 conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 32, 128, 98)  392         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 128, 98)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 32, 128, 14)  4116        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 32, 128, 14)  588         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 32, 128, 112) 0           concatenate_57[0][0]             \n",
      "                                                                 conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 32, 128, 112) 448         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 128, 112) 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 32, 128, 14)  1568        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 16, 64, 14)   0           conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 16, 64, 14)   56          average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 16, 64, 14)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 16, 64, 10)   420         activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 16, 64, 10)   300         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 16, 64, 24)   0           average_pooling2d_12[0][0]       \n",
      "                                                                 conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 16, 64, 24)   96          concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 16, 64, 24)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 16, 64, 10)   720         activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 16, 64, 10)   300         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 16, 64, 34)   0           concatenate_59[0][0]             \n",
      "                                                                 conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 16, 64, 34)   136         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 16, 64, 34)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 16, 64, 10)   1020        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 16, 64, 10)   300         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 16, 64, 44)   0           concatenate_60[0][0]             \n",
      "                                                                 conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 16, 64, 44)   176         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 16, 64, 44)   0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 16, 64, 10)   1320        activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 16, 64, 10)   300         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 16, 64, 54)   0           concatenate_61[0][0]             \n",
      "                                                                 conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 16, 64, 54)   216         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 16, 64, 54)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 16, 64, 10)   1620        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 16, 64, 10)   300         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 16, 64, 64)   0           concatenate_62[0][0]             \n",
      "                                                                 conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 16, 64, 64)   256         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 16, 64, 64)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 16, 64, 10)   1920        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 16, 64, 10)   300         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 16, 64, 74)   0           concatenate_63[0][0]             \n",
      "                                                                 conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 16, 64, 74)   296         concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 16, 64, 74)   0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 16, 64, 10)   2220        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 16, 64, 10)   300         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 16, 64, 84)   0           concatenate_64[0][0]             \n",
      "                                                                 conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 16, 64, 84)   336         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 16, 64, 84)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 16, 64, 10)   2520        activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 64, 10)   300         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 16, 64, 94)   0           concatenate_65[0][0]             \n",
      "                                                                 conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 16, 64, 94)   376         concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 16, 64, 94)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 64, 10)   2820        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 16, 64, 10)   300         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 16, 64, 104)  0           concatenate_66[0][0]             \n",
      "                                                                 conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 64, 104)  416         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 16, 64, 104)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 16, 64, 10)   3120        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 16, 64, 10)   300         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 16, 64, 114)  0           concatenate_67[0][0]             \n",
      "                                                                 conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 64, 114)  456         concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 16, 64, 114)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 64, 10)   3420        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 64, 10)   300         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 16, 64, 124)  0           concatenate_68[0][0]             \n",
      "                                                                 conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 64, 124)  496         concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 16, 64, 124)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 64, 10)   1240        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 8, 32, 10)    0           conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 32, 10)    40          average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 32, 10)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 4, 16, 10)    0           activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 640)          0           average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            641         flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 54,195\n",
      "Trainable params: 51,511\n",
      "Non-trainable params: 2,684\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the hyperparameters : growth rate in densenets\n",
    "\n",
    "# For densenet-blovk-1\n",
    "growth_rate_net1 = 14\n",
    "numLayers_net1 = 7\n",
    "\n",
    "# For densenet-block-2\n",
    "growth_rate_net2 = 10\n",
    "numLayers_net2 = 11\n",
    "\n",
    "# No dropout\n",
    "dropout_rate = 0.0\n",
    "\n",
    "input_shape=(32, 128, 3)\n",
    "\n",
    "input = Input(shape=input_shape)\n",
    "# First Layer\n",
    "First_Conv2D = Conv2D(growth_rate_net1, (3,3), use_bias=False ,padding='same',dilation_rate=2)(input)\n",
    "\n",
    "# First densenet block\n",
    "First_Block = add_denseblock(First_Conv2D, growth_rate_net1, numLayers_net1, dropout_rate)\n",
    "First_Transition = add_transition(First_Block, growth_rate_net1, dropout_rate)\n",
    "\n",
    "# Second densenet block\n",
    "Second_Block = add_denseblock(First_Transition, growth_rate_net2, numLayers_net2, dropout_rate) \n",
    "Second_Transition = add_transition(Second_Block, growth_rate_net2, dropout_rate)\n",
    "\n",
    "# Output Layer\n",
    "output = output_layer(Second_Transition)\n",
    "\n",
    "model = Model(inputs=[input], outputs=[output])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1788
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3115261,
     "status": "ok",
     "timestamp": 1530597257246,
     "user": {
      "displayName": "Suhan Shetty",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "108658629281572684125"
     },
     "user_tz": -330
    },
    "id": "NrExKS5aKV_v",
    "outputId": "c9534572-fc69-47b1-8f8b-7968ac4bc2d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compilation Successful\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/48 [==============================] - 82s 2s/step - loss: 0.1514 - val_loss: 0.1600\n",
      "Epoch 2/50\n",
      "49/48 [==============================] - 58s 1s/step - loss: 0.0826 - val_loss: 0.0852\n",
      "Epoch 3/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0815 - val_loss: 0.1081\n",
      "Epoch 4/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0768 - val_loss: 0.1022\n",
      "Epoch 5/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0732 - val_loss: 0.0745\n",
      "Epoch 6/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0723 - val_loss: 0.0684\n",
      "Epoch 7/50\n",
      "10/48 [=====>........................] - ETA: 38s - loss: 0.064849/48 [==============================] - 60s 1s/step - loss: 0.0705 - val_loss: 0.0594\n",
      "Epoch 8/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0696 - val_loss: 0.0799\n",
      "Epoch 9/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0711 - val_loss: 0.0868\n",
      "Epoch 10/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0665 - val_loss: 0.0588\n",
      "Epoch 11/50\n",
      "49/48 [==============================] - 59s 1s/step - loss: 0.0681 - val_loss: 0.0822\n",
      "Epoch 12/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0667 - val_loss: 0.1095\n",
      "Epoch 13/50\n",
      "11/48 [=====>........................] - ETA: 37s - loss: 0.066449/48 [==============================] - 60s 1s/step - loss: 0.0646 - val_loss: 0.0845\n",
      "Epoch 14/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0635 - val_loss: 0.0644\n",
      "Epoch 15/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0645 - val_loss: 0.1057\n",
      "Epoch 16/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0626 - val_loss: 0.0626\n",
      "Epoch 17/50\n",
      "49/48 [==============================] - 59s 1s/step - loss: 0.0643 - val_loss: 0.1089\n",
      "Epoch 18/50\n",
      "49/48 [==============================] - 59s 1s/step - loss: 0.0634 - val_loss: 0.0563\n",
      "Epoch 19/50\n",
      "10/48 [=====>........................] - ETA: 38s - loss: 0.060849/48 [==============================] - 59s 1s/step - loss: 0.0629 - val_loss: 0.0956\n",
      "Epoch 20/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0617 - val_loss: 0.0744\n",
      "Epoch 21/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0614 - val_loss: 0.0530\n",
      "Epoch 22/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0634 - val_loss: 0.0600\n",
      "Epoch 23/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0606 - val_loss: 0.0958\n",
      "Epoch 24/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0601 - val_loss: 0.1101\n",
      "Epoch 25/50\n",
      "10/48 [=====>........................] - ETA: 38s - loss: 0.060049/48 [==============================] - 60s 1s/step - loss: 0.0568 - val_loss: 0.0580\n",
      "Epoch 26/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0568 - val_loss: 0.0506\n",
      "Epoch 27/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0551 - val_loss: 0.0506\n",
      "Epoch 28/50\n",
      "49/48 [==============================] - 59s 1s/step - loss: 0.0534 - val_loss: 0.0488\n",
      "Epoch 29/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0546 - val_loss: 0.0515\n",
      "Epoch 30/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0540 - val_loss: 0.0478\n",
      "Epoch 31/50\n",
      "10/48 [=====>........................] - ETA: 38s - loss: 0.056949/48 [==============================] - 61s 1s/step - loss: 0.0540 - val_loss: 0.0492\n",
      "Epoch 32/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0543 - val_loss: 0.0488\n",
      "Epoch 33/50\n",
      "49/48 [==============================] - 63s 1s/step - loss: 0.0537 - val_loss: 0.0495\n",
      "Epoch 34/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0532 - val_loss: 0.0532\n",
      "Epoch 35/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0529 - val_loss: 0.0493\n",
      "Epoch 36/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0533 - val_loss: 0.0513\n",
      "Epoch 37/50\n",
      "10/48 [=====>........................] - ETA: 38s - loss: 0.056349/48 [==============================] - 61s 1s/step - loss: 0.0534 - val_loss: 0.0517\n",
      "Epoch 38/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0529 - val_loss: 0.0535\n",
      "Epoch 39/50\n",
      "49/48 [==============================] - 63s 1s/step - loss: 0.0526 - val_loss: 0.0510\n",
      "Epoch 40/50\n",
      "49/48 [==============================] - 63s 1s/step - loss: 0.0529 - val_loss: 0.0481\n",
      "Epoch 41/50\n",
      "49/48 [==============================] - 63s 1s/step - loss: 0.0514 - val_loss: 0.0486\n",
      "Epoch 42/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0533 - val_loss: 0.0465\n",
      "Epoch 43/50\n",
      "10/48 [=====>........................] - ETA: 38s - loss: 0.050049/48 [==============================] - 62s 1s/step - loss: 0.0520 - val_loss: 0.0473\n",
      "Epoch 44/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0528 - val_loss: 0.0565\n",
      "Epoch 45/50\n",
      "49/48 [==============================] - 63s 1s/step - loss: 0.0523 - val_loss: 0.0492\n",
      "Epoch 46/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0525 - val_loss: 0.0546\n",
      "Epoch 47/50\n",
      "49/48 [==============================] - 61s 1s/step - loss: 0.0519 - val_loss: 0.0488\n",
      "Epoch 48/50\n",
      "49/48 [==============================] - 60s 1s/step - loss: 0.0513 - val_loss: 0.0497\n",
      "Epoch 49/50\n",
      "10/48 [=====>........................] - ETA: 38s - loss: 0.054549/48 [==============================] - 60s 1s/step - loss: 0.0518 - val_loss: 0.0472\n",
      "Epoch 50/50\n",
      "49/48 [==============================] - 62s 1s/step - loss: 0.0503 - val_loss: 0.0469\n"
     ]
    }
   ],
   "source": [
    "# Determine the loss function, optimizer, compile and start training\n",
    "\n",
    "# Learning Rate Schedule: drop the learnig rate by 10 after every 25 epochs. Initial learning rate is 0.005\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.005\n",
    "    drop = 0.1\n",
    "    epochs_drop = 25.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "    math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# Callback for learning rate scheduling\n",
    "lrate = LearningRateScheduler(step_decay)  \n",
    "\n",
    "# Determine Loss function and Optimizer. SGD works equally well in this case.\n",
    "adm = keras.optimizers.Adam(lr = 0.0001,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile the model. Loss function: mean squared error\n",
    "model.compile(loss='mean_squared_error',optimizer=adm)\n",
    "print(\"Model Compilation Successful\")\n",
    "\n",
    "# Training \n",
    "\n",
    "epoch = 50\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generate_samples(df_train,local_data_path, batch_size),\n",
    "    steps_per_epoch=df_train.shape[0]/batch_size,\n",
    "    epochs=epoch,\n",
    "    callbacks=[lrate,WeightsLogger(root_path=local_project_path)],\n",
    "    validation_data=generate_samples(df_valid, local_data_path, augment=False),\n",
    "    validation_steps = df_valid.shape[0]/batch_size,\n",
    "    initial_epoch=0, verbose=1)\n",
    "\n",
    "# Save the model. \n",
    "model.save('model.h5')\n",
    "\n",
    "# If you require, save the .json form of the model\n",
    "with open('model.json', 'w') as file:\n",
    "    file.write(model.to_json())\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "#-------------------------------------------END OF CODE------------------------------------------------------------#\n",
    "#------------------------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training results:\n",
    "\n",
    "With the above-choice of CNN architecture and the hyper-parameters, the loss function was about 0.05 at the end of 50th epoch. As you can see in the above training summary, it hints at further reduction in loss function if we continue training longer. The loss function can be further reduced if you train longer. For this tutorial, the loss function that we have achieved is good enough. The range of output of the CNN is (-1,1) and it corresponds to steering angle (-25,25) in degrees. So, mean squared error of 0.05. in this case, means an absolute error of 5 degrees in prediction on an average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Testing\n",
    "\n",
    "Now we have the trained CNN model to predict steering angle of the car, we can run the simulator in the autonomous mode. To do this, we will use 'drive.py' and the saved CNN model 'model.h5'. Both of these files exist in 'e2e_project/'. \n",
    "\n",
    "Follow the below steps to run the simulator in the autonomous mode:\n",
    "- Open the simulator and click on autonmous mode. Choose track-1\n",
    "- Make sure 'drive.py' and 'model.h5' are in the local_project_path ('../e2e_project')\n",
    "- Execute drive.py\n",
    "\n",
    "A video of the autonomous drive:\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=cMEUmcrLJdw\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/cMEUmcrLJdw/0.jpg\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"540\" height=\"180\" border=\"10\" /></a>\n",
    "\n",
    "\n",
    "*drive.py* has a parameters that you can tune: MAX_SPEED. You can change this to suite your model. I am running the simulator on a CPU so a *MAX_SPEED=8* worked well for me. The MAX_SPEED really depends on how fast *drive.py* can interract with the simulator. If you have a GPU on your PC, then *drive.py* will be able to compute the steering angle prediction from the CNN model at high speed and change the control signals accordingly. So tune *MAX_SPEED* according to your PC. Note that complexity of the CNN network plays a role here. If you have very complex model, with a lot of parameters, then *drive.py* will take a lot of time of each predictiong, and hence will not be able to drive at higher speed. We have used only about 50k parameters in this model - thanks to 1xC-Cx1 convolution, dilatation rate, and non existence of fully-connected layers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis \n",
    "\n",
    "\n",
    "- With the dataset in hand and the above CNN model, the car was able to complete track-1 autonomously.\n",
    "\n",
    "\n",
    "- The CNN model involved only about 50k parameters. It is very efficient - thanks to dilated 1xC-Cx1 convolutions and non-existence of fully-connected layer.  \n",
    "\n",
    "\n",
    "- The CNN model did not use any fully-connected layers (dense layers before the output). I observed that addition of fully-connected layers after the convolution layers did not help in reducing the loss function. \n",
    "\n",
    "\n",
    "- 1xC-Cx1 dilated convolution helped to speed up the model prediction and training time. I am not aware of people using this type of convolution in designing end-to-end learning model for cars. It seems like a natural tool.\n",
    "\n",
    "\n",
    "\n",
    "#### Some Observations:\n",
    "\n",
    "\n",
    "- Addition of fully connected layers did not help in reducing cost\n",
    "\n",
    "- Addition of more densent blocks reduced the cost slightly. However, the project was constrained by two densenet blocks\n",
    "\n",
    "- Comparision of 1xC-Cx1 with CxC.: No significant influence. 1xC-Cx1 requires fewer parameters.\n",
    "\n",
    "- Learning rate schedule in the range 0.005 to 0.00005 seemed to work best\n",
    "\n",
    "\n",
    "#### Shortcomings and Possible Causes:\n",
    "\n",
    "\n",
    "- The maximum speed achieved in track-1 was 9mph. This can be imroved by some clever choice of parameterization of throttle in terms of speed. Running the simulator on a device with GPU will help speed up.\n",
    "\n",
    "\n",
    "- The data collected was poor. I did a lot of erroneous training and it effected the model performance to some extent. \n",
    "\n",
    "\n",
    "- Although the model performed well in track-1, it was not able to generalise well and the car could not drive in track-2(unseen data). This may be due to the poor quality of the data that I have collected. Ideally, we need to collect the data for different scenarios. The datset must include a lot of recovery mode driving - placing the car near the corner of the road and recoverinig from there to centre of the road. This requires some patience and effort during data collection. Also, I did not spend enoughtime in pre-processing of the data. A better strategy for data scrapping will signifacantly improve the performance of the model. Data collection, data preprocessing, and data augmentation plays most important role in deciding the model performance. The steering_angle_correction, image resizing are some of the key hyperparameters in data augmentation.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#### What more can be done?\n",
    "- Lot of scope for improvement in data collection, data preprocessing, and data augmentation \n",
    "\n",
    "- Predict steering, speed, brake altogether - data is already available\n",
    "It would be interesting to come up with a model to predict speed, brake along with steering angle. The data is already available from the simulator. So it is feasible. \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "final_6.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
